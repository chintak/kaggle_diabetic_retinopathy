{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The point of this notebook is to do a quick prediction on some sample images with a pretrained network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sys.path.append('../')\n",
    "sys.path.insert(0, '../models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import time\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "import pandas as p\n",
    "import lasagne as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import hms, architecture_string, get_img_ids_from_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "\n",
    "rcParams['figure.figsize'] = 16, 6\n",
    "# rcParams['text.color'] = 'red'\n",
    "# rcParams['xtick.color'] = 'red'\n",
    "# rcParams['ytick.color'] = 'red'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the dump of the trained network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_dump_path = '../dumps/2015_07_17_123003_PARAMSDUMP.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_model_data = pickle.load(open(param_dump_path, 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_model_data[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import basic_model as model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l_out, l_ins = model.build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn.layers.set_all_param_values(l_out, param_model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_data_store = {}\n",
    "model_data_store['chunk_size'] = model_data['chunk_size']\n",
    "model_data_store['batch_size'] = model_data['batch_size']\n",
    "model_data_store['data_loader_params'] = model_data['data_loader_params']\n",
    "model_data_store['paired_transfos'] = model_data['paired_transfos']\n",
    "print model_data.has_key('data_loader_no_transfos')\n",
    "print model_data.has_key('data_loader_default_transfo_params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extra_param_dump_path = '../dumps/2015_07_17_123003_EXTRAS.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(model_data_store, open(extra_param_dump_path, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dump_path = '../dumps/2015_07_17_123003.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_data = pickle.load(open(dump_path, 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_data['batch_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's set the in and output layers to some local vars.\n",
    "l_out = model_data['l_out']\n",
    "l_ins = model_data['l_ins']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some info about the architecture of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_arch = architecture_string(model_data['l_out'])\n",
    "# model_arch = ''\n",
    "\n",
    "num_params = nn.layers.count_params(l_out)\n",
    "model_arch += \"\\nNumber of parameters: %d.\\n\\n\" % num_params\n",
    "\n",
    "# Get some training/validation info.\n",
    "selected_keys = ['acc_eval_train', 'acc_eval_valid',\n",
    "                 'losses_eval_train', 'losses_eval_valid',\n",
    "                 'metric_eval_train', 'metric_eval_valid',\n",
    "                 'metric_cont_eval_train', 'metric_cont_eval_valid']\n",
    "model_metrics = {key: model_data[key]\n",
    "                 for key in selected_keys if key in model_data}\n",
    "\n",
    "res_df = p.DataFrame(model_metrics)\n",
    "\n",
    "model_arch += 'BEST/LAST KAPPA TRAIN: %.3f - %.3f.\\n' % (\n",
    "    res_df.metric_eval_train.max(),\n",
    "    res_df.metric_eval_train.iloc[-1]\n",
    ")\n",
    "model_arch += 'BEST/LAST KAPPA VALID: %.3f - %.3f.\\n' % (\n",
    "    res_df.metric_eval_valid.max(),\n",
    "    res_df.metric_eval_valid.iloc[-1]\n",
    ")\n",
    "\n",
    "model_arch += '\\nBEST/LAST ACC TRAIN: %.2f - %.2f.\\n' % (\n",
    "    res_df.acc_eval_train.max() * 100,\n",
    "    res_df.acc_eval_train.iloc[-1] * 100\n",
    ")\n",
    "\n",
    "model_arch += 'BEST/LAST ACC VALID: %.2f - %.2f.\\n' % (\n",
    "    res_df.acc_eval_valid.max() * 100,\n",
    "    res_df.acc_eval_valid.iloc[-1] * 100\n",
    ")\n",
    "\n",
    "model_arch += '\\nTOTAL TRAINING TIME: %s' % \\\n",
    "              hms(model_data['time_since_start'])\n",
    "\n",
    "print model_arch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* very long training time (80 hours!) is because of the (slow) AWS GPU and something special in the generator process which I only added at the end (the extra width cropping). You can get similar performance in (less than) 1 day with a GTX 980."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Extra note:* if you have read my [blog post](http://jeffreydf.github.io/diabetic-retinopathy-detection/) you might notice the accuracy being much higher here than at the model image at the end (around 84% vs 80%). This is not (really) because this model is better but because this model used a lower *y_pow* (namely, *y_pow=1*). *y_pow* specifies to which power to raise the predictions before calculating the loss (see losses.py) and higher *y_pow* (mostly *y_pow=2*) gives much lower accuracy (around 80%) but used to give higher kappa scores. In the end they seemed to give similar kappa scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some more kappa specific metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_conf_mat, hist_rater_a, \\\n",
    "        hist_rater_b, train_nom, \\\n",
    "        train_denom = model_data['metric_extra_eval_train'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valid_conf_mat, hist_rater_a, \\\n",
    "        hist_rater_b, valid_nom, \\\n",
    "        valid_denom = model_data['metric_extra_eval_valid'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Normalised train confusion matrix (with argmax decoding).\n",
    "print train_conf_mat / train_conf_mat.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Normalised validation confusion matrix (with argmax decoding).\n",
    "print valid_conf_mat / valid_conf_mat.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up some Theano / Lasagne things to get some predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chunk_size = model_data['chunk_size'] * 2\n",
    "batch_size = model_data['batch_size']\n",
    "\n",
    "print \"Batch size: %i.\" % batch_size\n",
    "print \"Chunk size: %i.\" % chunk_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output = nn.layers.get_output(l_out, deterministic=True)\n",
    "input_ndims = [len(nn.layers.get_output_shape(l_in))\n",
    "               for l_in in l_ins]\n",
    "xs_shared = [nn.utils.shared_empty(dim=ndim)\n",
    "             for ndim in input_ndims]\n",
    "idx = T.lscalar('idx')\n",
    "\n",
    "givens = {}\n",
    "for l_in, x_shared in zip(l_ins, xs_shared):\n",
    "    givens[l_in.input_var] = x_shared[idx * batch_size:(idx + 1) * batch_size]\n",
    "\n",
    "compute_output = theano.function(\n",
    "    [idx],\n",
    "    output,\n",
    "    givens=givens,\n",
    "    on_unused_input='ignore'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Do transformations per patient instead?\n",
    "if 'paired_transfos' in model_data:\n",
    "    paired_transfos = model_data['paired_transfos']\n",
    "else:\n",
    "    paired_transfos = False\n",
    "    \n",
    "print paired_transfos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xs_shared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to test on some train images, so loading the training set labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels = p.read_csv(os.path.join('../data/trainLabels.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print train_labels.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get all patient ids.\n",
    "patient_ids = sorted(set(get_img_ids_from_iter(train_labels.image)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_chunks = int(np.ceil((2 * len(patient_ids)) / float(chunk_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Where all the images are located: \n",
    "# it looks for [img_dir]/[patient_id]_[left or right].jpeg\n",
    "img_dir = '/home/ubuntu/digits-server/train/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the DataLoader to set up the parameters, you could replace it with something much simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from generators import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_loader = DataLoader()\n",
    "new_dataloader_params = model_data['data_loader_params']\n",
    "new_dataloader_params.update({'images_test': patient_ids})\n",
    "new_dataloader_params.update({'labels_test': train_labels.level.values})\n",
    "new_dataloader_params.update({'prefix_train': img_dir})\n",
    "data_loader.set_params(new_dataloader_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function is going to iterate over a test generator to get the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_pred(test_gen):\n",
    "    outputs = []\n",
    "\n",
    "    for e, (xs_chunk, chunk_shape, chunk_length) in enumerate(test_gen()):\n",
    "        num_batches_chunk = int(np.ceil(chunk_length / float(batch_size)))\n",
    "\n",
    "        print \"Chunk %i/%i\" % (e + 1, num_chunks)\n",
    "        print chunk_shape, chunk_length\n",
    "\n",
    "        print \"  load data onto GPU\"\n",
    "        for x_shared, x_chunk in zip(xs_shared, xs_chunk):\n",
    "            x_shared.set_value(x_chunk)\n",
    "\n",
    "        print \"  compute output in batches\"\n",
    "        outputs_chunk = []\n",
    "        for b in xrange(num_batches_chunk):\n",
    "            out = compute_output(b)\n",
    "            outputs_chunk.append(out)\n",
    "\n",
    "        outputs_chunk = np.vstack(outputs_chunk)\n",
    "        outputs_chunk = outputs_chunk[:chunk_length]\n",
    "\n",
    "        outputs.append(outputs_chunk)\n",
    "\n",
    "    return np.vstack(outputs), xs_chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the default \"no transformation\" parameters for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "no_transfo_params = model_data['data_loader_params']['no_transfo_params']\n",
    "\n",
    "print no_transfo_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And set up the test generator on the first 256 patients of the training set (512 images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The default gen with \"no transfos\".\n",
    "test_gen = lambda: data_loader.create_fixed_gen(\n",
    "    data_loader.images_test[:128*2],\n",
    "    chunk_size=chunk_size,\n",
    "    prefix_train=img_dir,\n",
    "    prefix_test=img_dir,\n",
    "    transfo_params=no_transfo_params,\n",
    "    paired_transfos=paired_transfos,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can get some predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "outputs_orig, chunk_orig = do_pred(test_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore some of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from metrics import continuous_kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outputs_labels = np.argmax(outputs_orig, axis=1)\n",
    "\n",
    "kappa_eval = continuous_kappa(\n",
    "                outputs_labels,\n",
    "                train_labels.level.values[:outputs_labels.shape[0]],\n",
    "            )\n",
    "\n",
    "metric, conf_mat, \\\n",
    "    hist_rater_a, hist_rater_b, \\\n",
    "    nom, denom = kappa_eval\n",
    "    \n",
    "print 'Kappa %.4f' % metric, '\\n'\n",
    "print conf_mat, '\\n'\n",
    "print nom, '\\n'\n",
    "print nom / nom.sum(), nom.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bit high of a kappa but this is because: \n",
    "\n",
    "1. There is a gap between the train and validation kappa.\n",
    "2. This is a small sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's discriminate between train / validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_imgs = set(data_loader.images_train_0)\n",
    "valid_idx = [0  if img in train_imgs else 1 for img in data_loader.images_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_preds = p.DataFrame([train_labels.image[:outputs_labels.shape[0]],\n",
    "                        outputs_labels,\n",
    "                        train_labels.level.values[:outputs_labels.shape[0]],\n",
    "                       np.repeat(valid_idx, 2)[:outputs_labels.shape[0]]]).T\n",
    "df_preds.columns = ['image', 'pred', 'true', 'valid']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The misclassifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_preds[df_preds.pred != df_preds.true]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some sample activations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diag_out = theano.function(\n",
    "    [idx],\n",
    "    nn.layers.get_output(nn.layers.get_all_layers(l_out), deterministic=True),\n",
    "    givens=givens,\n",
    "    on_unused_input=\"ignore\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diag_result = np.asarray(diag_out(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The input images.\n",
    "diag_result[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_rollaxis(im, figsize=(15, 15), \n",
    "                  zmuv_mean=data_loader.zmuv_mean, \n",
    "                  zmuv_std=data_loader.zmuv_std,\n",
    "                 norm=True, ax=None):\n",
    "    if not ax:\n",
    "        fig, ax = plt.subplots(1, figsize=figsize)\n",
    "        \n",
    "    if norm:\n",
    "        ax.imshow((zmuv_std[0] + 0.05) * np.rollaxis(im, 0, 3) + zmuv_mean[0])\n",
    "    else:\n",
    "        ax.imshow(np.rollaxis(im, 0, 3))\n",
    "        \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_rollaxis(diag_result[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do keep in mind, we work in \"chunks\" and only the last \"chunk\" is still loaded on the GPU.\n",
    "\n",
    "Since a chunk is 256 images, we can subset those predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_chunk = df_preds[-128*2:]\n",
    "df_chunk['idx'] = np.repeat(range(128), 2)\n",
    "\n",
    "print df_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To print some output for a layer. (Hacky / quick.)\n",
    "def print_output(layer_out, norm=False):\n",
    "    fig, ax = plt.subplots(1, 2, sharey=True, figsize=(15, 200))\n",
    "\n",
    "    for i, elem in enumerate(np.asarray(layer_out)[:2]):\n",
    "        print elem.shape\n",
    "        \n",
    "        if norm:\n",
    "            ax[i].imshow(np.concatenate(elem, axis=0), cmap=plt.cm.gray, \n",
    "                         vmin=np.asarray(layer_out).min(),\n",
    "                         vmax=np.asarray(layer_out).max())\n",
    "        else:        \n",
    "            ax[i].imshow(np.concatenate(elem, axis=0), cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, if we take index 8, we should see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_chunk[df_chunk.idx == idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_rollaxis(diag_result[0][2*idx+0])  # Left.\n",
    "plot_rollaxis(diag_result[0][2*idx+1])  # Right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the output of one filter of the first layer for the left eye of that patient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "\n",
    "ax.imshow(diag_result[1][2*idx+0][24], cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do this for some images with microaneurysms, most of the time you will see them getting \"detected\".\n",
    "\n",
    "You can then also follow this \"detection\" through the following layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the activations for both eyes for a certain layer, we can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_output(diag_result[4][2*idx:], norm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_chunk[df_chunk.idx == idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_rollaxis(diag_result[0][2*idx+0])  # Left.\n",
    "plot_rollaxis(diag_result[0][2*idx+1])  # Right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice **the camera artifacts**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "\n",
    "ax.imshow(diag_result[1][2*idx+0][10], cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_output(diag_result[3][2*idx:], norm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
